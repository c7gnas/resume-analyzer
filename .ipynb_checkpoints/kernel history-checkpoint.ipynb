{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b4f497",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "from flask_wtf import FlaskForm\n",
      "from wtforms import StringField, PasswordField, SubmitField\n",
      "from wtforms import ValidationError\n",
      "from wtforms.validators import DataRequired, Email, EqualTo\n",
      "\n",
      "\n",
      "class RegistrationForm(FlaskForm):\n",
      "    first_name = StringField('first_name', validators=[DataRequired()])\n",
      "    last_name = StringField('last_name', validators=[DataRequired()])\n",
      "    email = StringField('email', validators=[DataRequired(), Email()])\n",
      "    password = PasswordField('password', validators=[DataRequired(), EqualTo('confirm_pass', message='Passwords doesn\\'t match')])\n",
      "    confirm_pass = PasswordField('confirm_pass', validators=[DataRequired()])\n",
      "    submit = SubmitField('Sign Up')\n",
      "\n",
      "    def check_email(self, field):\n",
      "        if User.query.filter_by(email=field.data).first():\n",
      "            raise ValidationError(\"Email already registered\")\n",
      "\n",
      "\n",
      "class LoginForm(FlaskForm):\n",
      "    email = StringField('email', validators=[DataRequired(), Email()])\n",
      "    password = PasswordField('password', validators=[DataRequired()])\n",
      "    submit = SubmitField(\"Login\")\n",
      " 2/1:\n",
      "from flask import (Flask, render_template, redirect, request, url_for, flash, abort)\n",
      "\n",
      "\n",
      "@app.route('/',methods=['GET','POST'])\n",
      "def signup():\n",
      "    return render_template('sign_up.html', form=form)\n",
      "\n",
      "\n",
      "@app.route('/login', methods=['GET', 'POST'])\n",
      "def login():\n",
      "    return render_template('login.html', form=form)\n",
      "\n",
      "@app.route('/home')\n",
      "@login_required\n",
      "def home():\n",
      "   return render_template('home.html')\n",
      " 3/1:\n",
      "import os\n",
      "\n",
      "from flask import (Flask, render_template, redirect, request, url_for, flash, abort)\n",
      "from flask_login import login_user, login_required, logout_user\n",
      "\n",
      "from Model import app, db\n",
      "from Model.forms import LoginForm, RegistrationForm\n",
      "from Model.models import User\n",
      "\n",
      "\n",
      "# template_dir = os.path.abspath('View/templates')\n",
      "# static_dir = os.path.abspath('View/static')\n",
      "# app = Flask(__name__, static_folder=static_dir, template_folder=template_dir)\n",
      "# app = Flask(__name__)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@app.route('/',methods=['GET','POST'])\n",
      "def signup():\n",
      "   form = RegistrationForm()\n",
      "\n",
      "   if form.validate_on_submit():\n",
      "      user = User(first_name=form.first_name.data, last_name=form.last_name.data, email=form.email.data, password=form.password.data)\n",
      "\n",
      "      db.session.add(user)\n",
      "      db.session.commit()\n",
      "\n",
      "      return redirect(url_for('login'))\n",
      "\n",
      "   return render_template('sign_up.html', form=form)\n",
      "\n",
      "@app.route('/login', methods=['GET', 'POST'])\n",
      "def login():\n",
      "\n",
      "   form = LoginForm()\n",
      "   if form.validate_on_submit():\n",
      "      user = User.query.filter_by(email=form.email.data).first()\n",
      "      \n",
      "      if user.check_password(form.password.data) and user is not None:\n",
      "         login_user(user)\n",
      "         \n",
      "         flash(\"Successful\")\n",
      "         return redirect(url_for('home'))\n",
      "\n",
      "   return render_template('login.html', form=form)\n",
      "\n",
      "@app.route('/home')\n",
      "@login_required\n",
      "def home():\n",
      "   return render_template('home.html')\n",
      "\n",
      "@app.route('/logout')\n",
      "@login_required\n",
      "def logout():\n",
      "   logout_user()\n",
      "   return redirect(url_for('home'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug='True')\n",
      " 9/1:\n",
      "import spacy\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "|\n",
      " 9/2:\n",
      "import spacy\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "10/1:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "10/2:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "12/1:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "12/2: pip install PyPDF2\n",
      "12/3: pip install PyPDF2\n",
      "12/4: import PyPDF2\n",
      "12/5:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "12/6:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "16/1:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "16/2:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "16/3:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "16/4:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "16/5:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "16/6: pip install PyPDF2\n",
      "16/7: import PyPDF2\n",
      "16/8:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "16/9:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "16/10:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "16/11:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "16/12: pip install PyPDF2\n",
      "16/13:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "16/14: import PyPDF2\n",
      "18/1:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "18/2:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "18/3: import PyPDF2\n",
      "18/4:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "18/5:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "18/6:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    import en_core_web_sm\n",
      "    nlp = en_core_web_sm.load()\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "18/7:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "19/1:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "19/2:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "   #import en_core_web_sm\n",
      "    nlp = en_core_web_sm.load()\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "19/3:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "19/4: import PyPDF2\n",
      "19/5:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "19/6:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "19/7:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "   #import en_core_web_sm\n",
      "    nlp.load()\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "19/8:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "19/9:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    import en_core_web_sm\n",
      "    nlp = en_core_web_sm.load()\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "19/10:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "19/11:\n",
      "# Extract keywords from the text data\n",
      "keywords1 = extract_keywords([resume1_processed])\n",
      "keywords2 = extract_keywords([resume2_processed])\n",
      "19/12:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names_out()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "19/13:\n",
      "# Extract keywords from the text data\n",
      "keywords1 = extract_keywords([resume1_processed])\n",
      "keywords2 = extract_keywords([resume2_processed])\n",
      "19/14:\n",
      "# Compare the contents of the two resumes\n",
      "similarity_score = compare_resumes(resume1_processed, resume2_processed)\n",
      "19/15:\n",
      "# Cluster resumes and visualize the results\n",
      "df = pd.read_csv(\"resume_dataset.csv\")\n",
      "labels = cluster_resumes(df[\"text\"], 5)\n",
      "visualize_data(labels)\n",
      "19/16: from wordcloud import WordCloud\n",
      "20/1:\n",
      "# Compare the contents of the two resumes\n",
      "similarity_score = compare_resumes(resume1_processed, resume2_processed)\n",
      "print(similarity_score)\n",
      "21/1:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "21/2:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    import en_core_web_sm\n",
      "    nlp = en_core_web_sm.load()\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "21/3:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names_out()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "21/4: import PyPDF2\n",
      "21/5:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "21/6:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "21/7:\n",
      "# Extract keywords from the text data\n",
      "keywords1 = extract_keywords([resume1_processed])\n",
      "keywords2 = extract_keywords([resume2_processed])\n",
      "21/8:\n",
      "# Compare the contents of the two resumes\n",
      "similarity_score = compare_resumes(resume1_processed, resume2_processed)\n",
      "print(similarity_score)\n",
      "21/9: from wordcloud import WordCloud\n",
      "21/10:\n",
      "# Cluster resumes and visualize the results\n",
      "df = pd.read_csv(\"resume_dataset.csv\")\n",
      "labels = cluster_resumes(df[\"text\"], 5)\n",
      "visualize_data(labels)\n",
      "21/11:\n",
      "# Cluster resumes and visualize the results\n",
      "df = pd.read_csv(\"resume_dataset.csv\")\n",
      "df.head()\n",
      "21/12:\n",
      "# Define a function to extract named entities from the keywords\n",
      "def extract_entities(text):\n",
      "    doc = nlp(text)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        entities.append(ent.text)\n",
      "    return entities\n",
      "23/1:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "23/2:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    import en_core_web_sm\n",
      "    nlp = en_core_web_sm.load()\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "23/3:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names_out()\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "23/4: import PyPDF2\n",
      "23/5:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "23/6:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "23/7:\n",
      "# Extract keywords from the text data\n",
      "keywords1 = extract_keywords([resume1_processed])\n",
      "keywords2 = extract_keywords([resume2_processed])\n",
      "23/8:\n",
      "# Compare the contents of the two resumes\n",
      "similarity_score = compare_resumes(resume1_processed, resume2_processed)\n",
      "print(similarity_score)\n",
      "23/9:\n",
      "# Cluster resumes and visualize the results\n",
      "df = pd.read_csv(\"resume_dataset.csv\")\n",
      "df.head()\n",
      "23/10:\n",
      "# Define a function to extract named entities from the keywords\n",
      "def extract_entities(text):\n",
      "    doc = nlp(text)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        entities.append(ent.text)\n",
      "    return entities\n",
      "23/11:\n",
      "# Extract keywords and named entities from the preprocessed resume text\n",
      "keywords1, keywords2 = extract_keywords(resume1_processed), extract_keywords(resume2_processed)\n",
      "entities1, entities2 = extract_entities(\" \".join(keywords1)), extract_entities(\" \".join(keywords2))\n",
      "23/12:\n",
      "# Extract keywords and named entities from the preprocessed resume text\n",
      "keywords1, keywords2 = extract_keywords(resume1_processed), extract_keywords(resume2_processed)\n",
      "entities1, entities2 = extract_entities(\" \".join(keywords1)), extract_entities(\" \".join(keywords2))\n",
      "23/13:\n",
      "# Extract keywords and named entities from the preprocessed resume text\n",
      "entities1, entities2 = extract_entities(\" \".join(keywords1)), extract_entities(\" \".join(keywords2))\n",
      "25/1:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "25/2:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    import en_core_web_sm\n",
      "    nlp = en_core_web_sm.load()\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "25/3:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names_out()\n",
      "\n",
      "# Define a function to extract named entities from the keywords\n",
      "def extract_entities(text):\n",
      "    doc = nlp(text)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        entities.append(ent.text)\n",
      "    return entities\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "25/4: import PyPDF2\n",
      "25/5:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "25/6:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "25/7:\n",
      "# Extract keywords from the text data\n",
      "keywords1 = extract_keywords([resume1_processed])\n",
      "keywords2 = extract_keywords([resume2_processed])\n",
      "25/8:\n",
      "# Compare the contents of the two resumes\n",
      "similarity_score = compare_resumes(resume1_processed, resume2_processed)\n",
      "print(similarity_score)\n",
      "25/9:\n",
      "# Cluster resumes and visualize the results\n",
      "df = pd.read_csv(\"resume_dataset.csv\")\n",
      "df.head()\n",
      "25/10:\n",
      "# Define a function to extract named entities from the keywords\n",
      "def extract_entities(text):\n",
      "    doc = nlp(text)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        entities.append(ent.text)\n",
      "    return entities\n",
      "25/11:\n",
      "# Extract keywords and named entities from the preprocessed resume text\n",
      "entities1, entities2 = extract_entities(\" \".join(keywords1)), extract_entities(\" \".join(keywords2))\n",
      "25/12:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "25/13:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names_out()\n",
      "\n",
      "# Define a function to extract named entities from the keywords\n",
      "def extract_entities(text):\n",
      "    doc = nlp(text)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        entities.append(ent.text)\n",
      "    return entities\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "25/14: import PyPDF2\n",
      "25/15:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "25/16:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "25/17:\n",
      "# Extract keywords from the text data\n",
      "keywords1 = extract_keywords([resume1_processed])\n",
      "keywords2 = extract_keywords([resume2_processed])\n",
      "25/18:\n",
      "# Compare the contents of the two resumes\n",
      "similarity_score = compare_resumes(resume1_processed, resume2_processed)\n",
      "print(similarity_score)\n",
      "25/19:\n",
      "# Extract keywords and named entities from the preprocessed resume text\n",
      "entities1, entities2 = extract_entities(\" \".join(keywords1)), extract_entities(\" \".join(keywords2))\n",
      "25/20:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names_out()\n",
      "\n",
      "# Define a function to extract named entities from the keywords\n",
      "def extract_entities(text):\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    import en_core_web_sm\n",
      "    nlp = en_core_web_sm.load()\n",
      "    doc = nlp(text)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        entities.append(ent.text)\n",
      "    return entities\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "25/21:\n",
      "# Extract keywords and named entities from the preprocessed resume text\n",
      "entities1, entities2 = extract_entities(\" \".join(keywords1)), extract_entities(\" \".join(keywords2))\n",
      "25/22:\n",
      "import gensim\n",
      "from gensim import corpora\n",
      "25/23:\n",
      "# Preprocess the resume text data\n",
      "resume_text = df['Resume_str']\n",
      "resume_text_processed = []\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "    import en_core_web_sm\n",
      "    nlp = en_core_web_sm.load()\n",
      "for text in resume_text:\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
      "    resume_text_processed.append(tokens)\n",
      "\n",
      "# Create a dictionary of the processed resume text data\n",
      "dictionary = corpora.Dictionary(resume_text_processed)\n",
      "\n",
      "# Create a corpus of the processed resume text data\n",
      "corpus = [dictionary.doc2bow(tokens) for tokens in resume_text_processed]\n",
      "25/24:\n",
      "# Preprocess the resume text data\n",
      "resume_text = df['Resume_str']\n",
      "resume_text_processed = []\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "for text in resume_text:\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
      "    resume_text_processed.append(tokens)\n",
      "\n",
      "# Create a dictionary of the processed resume text data\n",
      "dictionary = corpora.Dictionary(resume_text_processed)\n",
      "\n",
      "# Create a corpus of the processed resume text data\n",
      "corpus = [dictionary.doc2bow(tokens) for tokens in resume_text_processed]\n",
      "26/1:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "26/2:\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "26/3:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "26/4:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names_out()\n",
      "\n",
      "# Define a function to extract named entities from the keywords\n",
      "def extract_entities(text):\n",
      "    doc = nlp(text)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        entities.append(ent.text)\n",
      "    return entities\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "26/5: import PyPDF2\n",
      "26/6:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "26/7:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "26/8:\n",
      "# Extract keywords from the text data\n",
      "keywords1 = extract_keywords([resume1_processed])\n",
      "keywords2 = extract_keywords([resume2_processed])\n",
      "26/9:\n",
      "# Compare the contents of the two resumes\n",
      "similarity_score = compare_resumes(resume1_processed, resume2_processed)\n",
      "print(similarity_score)\n",
      "26/10:\n",
      "# Cluster resumes and visualize the results\n",
      "df = pd.read_csv(\"resume_dataset.csv\")\n",
      "df.head()\n",
      "26/11:\n",
      "# Extract keywords and named entities from the preprocessed resume text\n",
      "entities1, entities2 = extract_entities(\" \".join(keywords1)), extract_entities(\" \".join(keywords2))\n",
      "26/12:\n",
      "# Preprocess the resume text data\n",
      "resume_text = df['Resume_str']\n",
      "resume_text_processed = []\n",
      "for text in resume_text:\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
      "    resume_text_processed.append(tokens)\n",
      "\n",
      "# Create a dictionary of the processed resume text data\n",
      "dictionary = corpora.Dictionary(resume_text_processed)\n",
      "\n",
      "# Create a corpus of the processed resume text data\n",
      "corpus = [dictionary.doc2bow(tokens) for tokens in resume_text_processed]\n",
      "26/13:\n",
      "import gensim\n",
      "from gensim import corpora\n",
      "26/14: print(df.len())\n",
      "26/15: print(df.size())\n",
      "26/16: print(len(df))\n",
      "26/17:\n",
      "# Preprocess the resume text data\n",
      "resume_text = df['Resume_str']\n",
      "resume_text_processed = []\n",
      "for text in resume_text:\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
      "    resume_text_processed.append(tokens)\n",
      "\n",
      "# Create a dictionary of the processed resume text data\n",
      "dictionary = corpora.Dictionary(resume_text_processed)\n",
      "\n",
      "# Create a corpus of the processed resume text data\n",
      "corpus = [dictionary.doc2bow(tokens) for tokens in resume_text_processed]\n",
      "26/18:\n",
      "# Perform LDA topic modeling on the corpus\n",
      "num_topics = 10\n",
      "lda_model = gensim.models.ld\n",
      "26/19:\n",
      "# Perform LDA topic modeling on the corpus\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
      "                                            id2word=dictionary,\n",
      "                                            num_topics=num_topics,\n",
      "                                            passes=10,\n",
      "                                            alpha='auto',\n",
      "                                            per_word_topics=True)\n",
      "26/20:\n",
      "import pyLDAvis.gensim_models as gensimvis\n",
      "import pyLDAvis\n",
      "26/21:\n",
      "# Visualize the LDA model\n",
      "vis = gensimvis.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
      "pyLDAvis.display(vis)\n",
      "26/22: print(resume1_processed)\n",
      "26/23: print(keywords1)\n",
      "26/24:\n",
      "# Cluster resumes and visualize the results\n",
      "df = pd.read_csv(\"resume_dataset.csv\")\n",
      "labels = cluster_resumes(df[\"Resume_str\"], 5)\n",
      "visualize_data(labels)\n",
      "26/25:\n",
      "# Combine cosine similarity scores into a single dataframe\n",
      "df2 = pd.DataFrame({'Keyword': keywords, 'resume1_processed': scores_1, 'resume2_processed': scores_2})\n",
      "\n",
      "# Create a pairplot of the scores\n",
      "sns.pairplot(df2, diag_kind='hist')\n",
      "26/26:\n",
      "# Combine cosine similarity scores into a single dataframe\n",
      "df2 = pd.DataFrame({'Keyword': keywords1, 'resume1_processed': scores_1, 'resume2_processed': scores_2})\n",
      "\n",
      "# Create a pairplot of the scores\n",
      "sns.pairplot(df2, diag_kind='hist')\n",
      "26/27: print(entities1)\n",
      "26/28: from sklearn.cluster import KMeans\n",
      "26/29:\n",
      "# Cluster resumes and visualize the results\n",
      "df = pd.read_csv(\"resume_dataset.csv\")\n",
      "labels = cluster_resumes(df[\"Resume_str\"], 5)\n",
      "visualize_data(labels)\n",
      "26/30:\n",
      "# Convert the scores to a pandas DataFrame for visualization\n",
      "df2 = pd.DataFrame(similarity_score)\n",
      "\n",
      "# Generate the heatmap plot using seaborn\n",
      "sns.heatmap(df, annot=True, cmap='coolwarm')\n",
      "plt.title('Cosine Similarity Scores between Resumes')\n",
      "plt.xlabel('Resume IDs')\n",
      "plt.ylabel('Resume IDs')\n",
      "plt.show()\n",
      "26/31:\n",
      "# Convert the scores to a pandas DataFrame for visualization\n",
      "def compVis_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "    df2 = pd.DataFrame([vectors[0]], [vectors[1]] )\n",
      "    sns.heatmap(df, annot=True, cmap='coolwarm')\n",
      "    plt.title('Cosine Similarity Scores between Resumes')\n",
      "    plt.xlabel('Resume IDs')\n",
      "    plt.ylabel('Resume IDs')\n",
      "    plt.show()\n",
      "26/32: compVis_resumes(resume1_processed, resume2_processed)\n",
      "26/33:\n",
      "# Convert the scores to a pandas DataFrame for visualization\n",
      "def compVis_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "    df2 = pd.DataFrame([vectors[0]], [vectors[1]] )\n",
      "    sns.heatmap(df2, annot=True, cmap='coolwarm')\n",
      "    plt.title('Cosine Similarity Scores between Resumes')\n",
      "    plt.xlabel('Resume IDs')\n",
      "    plt.ylabel('Resume IDs')\n",
      "    plt.show()\n",
      "26/34: compVis_resumes(resume1_processed, resume2_processed)\n",
      "27/1:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.cluster import KMeans\n",
      "import PyPDF2\n",
      "27/2:\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "27/3:\n",
      "# Define functions for preprocessing and analysis\n",
      "def preprocess(text):\n",
      "    # Preprocess the text data by removing stop words, punctuation, etc.\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
      "    return \" \".join(tokens)\n",
      "27/4:\n",
      "# Define functions for preprocessing and analysis\n",
      "def extract_keywords(text):\n",
      "    # Extract relevant keywords and phrases from the text data\n",
      "    cv = CountVectorizer()\n",
      "    word_count_vector = cv.fit_transform(text)\n",
      "    return cv.get_feature_names_out()\n",
      "\n",
      "# Define a function to extract named entities from the keywords\n",
      "def extract_entities(text):\n",
      "    doc = nlp(text)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        entities.append(ent.text)\n",
      "    return entities\n",
      "\n",
      "def compare_resumes(resume1, resume2):\n",
      "    # Compare the contents of two resumes using cosine similarity\n",
      "    vectorizer = CountVectorizer().fit_transform([resume1, resume2])\n",
      "    vectors = vectorizer.toarray()\n",
      "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
      "\n",
      "def cluster_resumes(data, num_clusters):\n",
      "    # Cluster resumes based on certain features using K-means clustering\n",
      "    vectorizer = CountVectorizer()\n",
      "    X = vectorizer.fit_transform(data)\n",
      "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
      "    return kmeans.labels_\n",
      "\n",
      "def visualize_data(data):\n",
      "    # Visualize the results of the analysis using Seaborn and Matplotlib\n",
      "    sns.histplot(data)\n",
      "    plt.show()\n",
      "27/5:\n",
      "# Define a function to read in a PDF file and extract the text\n",
      "def read_pdf(filepath):\n",
      "    with open(filepath, 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "        resume_text = \"\"\n",
      "        for page in pdf_reader.pages:\n",
      "            resume_text += page.extract_text()\n",
      "    return resume_text\n",
      "27/6:\n",
      "# Read in the resume PDF files and preprocess the text data\n",
      "resume1 = read_pdf(\"resume1.pdf\")\n",
      "resume2 = read_pdf(\"resume2.pdf\")\n",
      "resume1_processed = preprocess(resume1)\n",
      "resume2_processed = preprocess(resume2)\n",
      "27/7: print(resume1_processed)\n",
      "27/8:\n",
      "# Extract keywords from the text data\n",
      "keywords1 = extract_keywords([resume1_processed])\n",
      "keywords2 = extract_keywords([resume2_processed])\n",
      "27/9:\n",
      "# Create a dataframe with the keyword data\n",
      "dataa = {'Resume 1': keywords1, 'Resume 2': keywords2}\n",
      "df2 = pd.DataFrame(data, index=['Keywords'])\n",
      "\n",
      "# Use TfidfVectorizer to vectorize the keyword data\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(df2.loc['Keywords'])\n",
      "\n",
      "# Use KMeans clustering to group the keywords into clusters\n",
      "kmeans = KMeans(n_clusters=3).fit(X)\n",
      "\n",
      "# Add the cluster labels to the dataframe\n",
      "df2['Cluster Labels'] = kmeans.labels_\n",
      "\n",
      "# Plot a heatmap to visualize the clusters\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.heatmap(X.toarray(), cmap='Blues', xticklabels=vectorizer.get_feature_names(), yticklabels=df.columns[:-1], cbar=False)\n",
      "plt.title('Keyword Clusters')\n",
      "plt.show()\n",
      "27/10:\n",
      "# Create a dataframe with the keyword data\n",
      "dataa = {'Resume 1': keywords1, 'Resume 2': keywords2}\n",
      "df2 = pd.DataFrame(dataa, index=['Keywords'])\n",
      "\n",
      "# Use TfidfVectorizer to vectorize the keyword data\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(df2.loc['Keywords'])\n",
      "\n",
      "# Use KMeans clustering to group the keywords into clusters\n",
      "kmeans = KMeans(n_clusters=3).fit(X)\n",
      "\n",
      "# Add the cluster labels to the dataframe\n",
      "df2['Cluster Labels'] = kmeans.labels_\n",
      "\n",
      "# Plot a heatmap to visualize the clusters\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.heatmap(X.toarray(), cmap='Blues', xticklabels=vectorizer.get_feature_names(), yticklabels=df.columns[:-1], cbar=False)\n",
      "plt.title('Keyword Clusters')\n",
      "plt.show()\n",
      "27/11:\n",
      "# Create a dataframe with the keyword data\n",
      "dataa = {'Resume 1': keywords1, 'Resume 2': keywords2}\n",
      "df2 = pd.DataFrame(dataa, index=['Keywords'])\n",
      "\n",
      "# Use TfidfVectorizer to vectorize the keyword data\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(df2.loc['Keywords'])\n",
      "\n",
      "# Use KMeans clustering to group the keywords into clusters\n",
      "kmeans = KMeans(n_clusters=3).fit(X)\n",
      "\n",
      "# Add the cluster labels to the dataframe\n",
      "df2['Cluster Labels'] = kmeans.labels_\n",
      "\n",
      "# Plot a heatmap to visualize the clusters\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.heatmap(X.toarray(), cmap='Blues', xticklabels=vectorizer.get_feature_names(), yticklabels=df.columns[:-1], cbar=False)\n",
      "plt.title('Keyword Clusters')\n",
      "plt.show()\n",
      "27/12: print(keywords1)\n",
      "27/13: print(len(keywords1))\n",
      "27/14:\n",
      "# Create a dataframe with the keyword data\n",
      "dataa = {'Resume 1': keywords1, 'Resume 2': keywords2}\n",
      "df2 = pd.DataFrame(dataa, index=['Keywords'])\n",
      "27/15:\n",
      "# Create a dataframe with the keyword data\n",
      "dataa = {'Resume 1': keywords1, 'Resume 2': keywords2}\n",
      "print(dataa)\n",
      "27/16: print(len(entities1)\n",
      "27/17: print(len(entities1)\n",
      "27/18: print(len(entities1))\n",
      "27/19:\n",
      "# Use TfidfVectorizer to vectorize the keyword data\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(df2.loc['Keywords'])\n",
      "\n",
      "# Use KMeans clustering to group the keywords into clusters\n",
      "kmeans = KMeans(n_clusters=3).fit(X)\n",
      "\n",
      "# Add the cluster labels to the dataframe\n",
      "df2['Cluster Labels'] = kmeans.labels_\n",
      "\n",
      "# Plot a heatmap to visualize the clusters\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.heatmap(X.toarray(), cmap='Blues', xticklabels=vectorizer.get_feature_names(), yticklabels=df.columns[:-1], cbar=False)\n",
      "plt.title('Keyword Clusters')\n",
      "plt.show()\n",
      "27/20:\n",
      "# Compare the contents of the two resumes\n",
      "similarity_score = compare_resumes(resume1_processed, resume2_processed)\n",
      "print(similarity_score)\n",
      "27/21:\n",
      "# Extract keywords and named entities from the preprocessed resume text\n",
      "entities1, entities2 = extract_entities(\" \".join(keywords1)), extract_entities(\" \".join(keywords2))\n",
      "27/22: print(len(entities1))\n",
      "27/23: print(entities1)\n",
      "27/24:\n",
      "# Create a dataframe with the keyword data\n",
      "dataa = {'Resume 1': entities1, 'Resume 2': entities2}\n",
      "print(dataa)\n",
      "27/25:\n",
      "# Use TfidfVectorizer to vectorize the keyword data\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(df2.loc['Keywords'])\n",
      "\n",
      "# Use KMeans clustering to group the keywords into clusters\n",
      "kmeans = KMeans(n_clusters=3).fit(X)\n",
      "\n",
      "# Add the cluster labels to the dataframe\n",
      "df2['Cluster Labels'] = kmeans.labels_\n",
      "\n",
      "# Plot a heatmap to visualize the clusters\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.heatmap(X.toarray(), cmap='Blues', xticklabels=vectorizer.get_feature_names(), yticklabels=df.columns[:-1], cbar=False)\n",
      "plt.title('Keyword Clusters')\n",
      "plt.show()\n",
      "27/26:\n",
      "# Create a dataframe with the keyword data\n",
      "dataa = {'Resume 1': entities1, 'Resume 2': entities2}\n",
      "print(dataa)\n",
      "df2 = pd.DataFrame(dataa, index=['Keywords'])\n",
      "27/27:\n",
      "import gensim\n",
      "from gensim import corpora\n",
      "27/28:\n",
      "# Preprocess the resume text data\n",
      "resume_text = df['Resume_str']\n",
      "resume_text_processed = []\n",
      "for text in resume_text:\n",
      "    doc = nlp(text)\n",
      "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
      "    resume_text_processed.append(tokens)\n",
      "\n",
      "# Create a dictionary of the processed resume text data\n",
      "dictionary = corpora.Dictionary(resume_text_processed)\n",
      "\n",
      "# Create a corpus of the processed resume text data\n",
      "corpus = [dictionary.doc2bow(tokens) for tokens in resume_text_processed]\n",
      "27/29:\n",
      "import gensim\n",
      "from gensim import corpora\n",
      "27/30:\n",
      "# Perform LDA topic modeling on the corpus\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
      "                                            id2word=dictionary,\n",
      "                                            num_topics=num_topics,\n",
      "                                            passes=10,\n",
      "                                            alpha='auto',\n",
      "                                            per_word_topics=True)\n",
      "27/31:\n",
      "# Import necessary libraries\n",
      "import spacy\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "   1: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacyy",
   "language": "python",
   "name": "spacyy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
